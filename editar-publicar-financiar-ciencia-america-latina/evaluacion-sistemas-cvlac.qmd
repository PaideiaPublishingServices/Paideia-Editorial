---
title: "Evaluating Data Integrity to Measure Scientific Output in Colombia"
author:
  - name: Juan Sebastián Gonzalez Sanabria
    orcid: 0000-0002-1024-6077
    email: juansebastian.gonzalez@uptc.edu.co
    affiliation:
       - name: Universidad Pedagógica y Tecnológica de Colombia
         city: Tunja
         state: Boyacá
         country: CO
         url: https://ror.org/04vdmbk59
         ror: https://ror.org/04vdmbk59
  - name: Elena Verdu
    orcid: 0000-0002-3040-7077
    email: elena.verdu@unir.net
    affiliation:
       - name: Universidad Internacional de La Rioja
         city: Logroño
         state: La Rioja
         country: ES
         url: https://ror.org/029gnnp81
         ror: https://ror.org/029gnnp81
  - name: Xiomara Blanco
    orcid: 0000-0001-9947-748X
    email: xiomarapatricia.blanco@unir.net
    affiliation:
       - name: Universidad Internacional de La Rioja
         city: Logroño
         state: La Rioja
         country: ES
         url: https://ror.org/029gnnp81
         ror: https://ror.org/029gnnp81
  - name: José Texier
    orcid: 0000-0003-0176-6625
    email: texier@laccei.org
    affiliation:
       - name: Latin American and Caribbean Consortium of Engineering Institutions
         city: Boca Raton
         state: Florida
         country: US
         url: https://www.laccei.org
         ror: https://www.laccei.org
  - name: Samuel Ruiz
    email: samuel.ruiz@uptc.edu.co
    affiliation:
       - name: Universidad Pedagógica y Tecnológica de Colombia
         city: Tunja
         state: Boyacá
         country: CO
         url: https://ror.org/04vdmbk59
         ror: https://ror.org/04vdmbk59
  - name: Esteban Novoa
    email: esteban.novoa@uptc.edu.co
    affiliation:
       - name: Universidad Pedagógica y Tecnológica de Colombia
         city: Tunja
         state: Boyacá
         country: CO
         url: https://ror.org/04vdmbk59
         ror: https://ror.org/04vdmbk59
date: 9/10/2025
abstract: |
  Information systems on academic CVs have become a source of data for governments to make strategic decisions regarding investments. However, these systems prove to be insufficient when the information is either incorrect or incomplete. This work proposes a tool to validate the research metadata stored in the Colombian researcher curriculum vitae system (CvLAC). To this effect, work focusing on data extraction, manipulation, and cleaning was carried out so as to perform analyses, studies, and validations using international systems like Crossref. Significant errors including inconsistent or misreported data were found; for instance, authors and data that do not correspond to existing articles or products. Identifying these errors will contribute to enhancing research evaluation processes and optimizing targeted investments.
keywords: 
   - science measurement
   - scientific productivity
   - data quality
   - interoperability
   - metadata
license: "CC-BY"
doi: 10.62059/editorial.l001.c8
citation:
  type: chapter
  container-title: "Evaluating the quality of data in academic curriculum vitae systems: the case of the Colombian CvLAC system"
  doi: 10.62059/editorial.l001.c8
lang: en
reference-section-title: "References"
format:
  pdf:
    include-in-header:
      text: |
        \usepackage{array}
        \usepackage{longtable}
        \usepackage{rotating}
        \usepackage[english]{babel}
  html:
     include-in-header: header-evaluacion-cvlac.html 
---

\chapterdoi{10.62059/editorial.l001.c8}
\chapterauthors{Juan Sebastián Gonzalez Sanabria$^1$, Elena Verdu$^2$, Xiomara Blanco$^2$, José Texier$^3$, Samuel Ruiz$^1$, Esteban Novoa$^1$}
\chapteraffiliations{
    $^1$ Universidad Pedagógica y Tecnológica de Colombia, Tunja, Boyacá, CO\\
    $^2$ Universidad Internacional de La Rioja, Logroño, La Rioja, ES\\
    $^3$ Latin American and Caribbean Consortium of Engineering Institutions, Boca Raton, Florida, US
}

\creditcontributionseng{
\textbf{Juan Sebastián Gonzalez Sanabria:} Conceptualization, Formal analysis, Funding acquisition, Investigation, Methodology, Software, Validation, Writing - original draft\\
\textbf{Elena Verdu:} Investigation, Methodology, Validation, Writing - review and editing\\
\textbf{Xiomara Blanco:} Investigation, Methodology, Validation, Writing - review and editing\\
\textbf{José Texier:} Investigation, Methodology, Validation, Writing - review and editing\\
\textbf{Samuel Ruiz:} Formal analysis, Methodology, Software, Writing - review and editing\\
\textbf{Esteban Novoa:} Formal analysis, Methodology, Software, Writing - review and editing
}

\renewcommand{\tablename}{Table}
\renewcommand{\figurename}{Figure}

## Introduction {.unnumbered}

While there is no single, universal CV format, both international and local governmental and organizational systems offer guidelines when writing a researcher's curriculum vitae (CV) so as to ensure correct information management and registry. On that account, certain features should be observed, namely, i) use of metadata: standardizing information storage with Dublin Core, ecite, among others, is key to attaining both optimal retrieval and clear display; ii) validation: verifying and evaluating the platform is paramount; iii) integration with international systems: integrating the system into ORCID, SciELO, LILACS, Scopus, or other duly recognized systems following integration best practices; iv) exporting: saving available information in digital export formats for later study and analysis [@VallesCoral2019].

As stated by @Riggio2017, "the international databases used to measure and compare the scientific performance and impact of countries, [the Web of Science and Scopus citation indices] have low coverage regarding Latin American journals and Spanish publications, which [...] influences the visibility of the scientific results of this region as well as the low production indices they obtain when analyzing data from these sources". This shows the limitations or biases in the assessment of research quality or impact on the region compared with European or North American systems.

CvLAC (Curriculum Vitae for Latin America and the Caribbean) is the system used in Colombia to register resumes of both national and foreign researchers residing in the country [@MinisterioCiencia2023]. The platform is managed by the Colombian Ministry of Science, Technology, and Innovation, and includes information on conference presentations, papers, projects and other research activities. Exploring this system, which is the only one used in Colombia to record national scientific production, led to the identification of an issue regarding its information structure. Furthermore, certain difficulties in verifying the quality and truthfulness of the registered data were diagnosed.

In order to validate the aspects contemplated by different systems of academic CVs in Latin America, each was characterized including entities and information sources. @tbl-cvlac1 presents a comparison of these systems.

| Country | Metadata standard | Validation | Integration with international systems | Exporting format | National repository |
|---------|------------------|------------|----------------------------------------|------------------|-------------------|
| **Argentina (SIC)** | -- | Automatic (External database) | -- | PDF | National digital repository system |
| **Brazil (Lattes)** | DC | Automatic (DOI) | SciELO, LILACS, Scopus, Crossref | PDF, XML | -- |
| **Colombia (CvLAC)** | -- | Manual (InstiLAC) | -- | PDF | -- |
| **Ecuador (CVN)** | -- | -- | -- | -- | Open-Access Repositories of Ecuador |
| **Mexico (SNI)** | -- | -- | -- | -- | CTI national repository |
| **Panama (SNI)** | -- | -- | ORCID | -- | Open-Access Scientific Repository of Portugal |
| **Paraguay (PIP)** | DC | -- | -- | PDF | CONACYT institutional repository |
| **Peru (DINA)** | DC | -- | -- | PDF, RTF, RefWorks | National repository of the CONCYTEC |
| **Venezuela (CvLAC)** | -- | -- | -- | -- | -- |

: Latin American academic CV systems. {#tbl-cvlac1}

Comparison based on the information available on the website and on the study of each system, has revealed obsolescence concerning availability, management and validation of most systems, with the exception of the Brazilian comprehensive CV system --The Lattes Platform - as it offers greater data reliability due to its validation process and its interoperability with international systems (e.g., Crossref or Scopus).

Latin American Systems often show a lack of integration with international sources of information or databases like Crossref [@Crossref2023] which, if used, would allow governments to validate part of the registered information and obtain real data to measure their installed research capabilities.

It is noteworthy to mention that while Brazil, Peru, and Paraguay apply metadata standards like Dublin Core, which facilitate the extraction of reported data for later study and analysis, countries like Ecuador and Venezuela lack governmental systems dedicated to managing research information.

Previous research works on the Colombian case have used tools such as CvLAC-GrupLAC Extractor [@MosqueraPerdomo2023] or Product Recommender Systems [@GaleanoPrada] which offer functionalities for extracting information registered in CvLAC or GrupLAC. In spite of that, lack of data validation is one of the inherent difficulties within the system since it perpetuates data integrity problems; for instance, when tools extract or analyze the information the way it was originally registered. As the existence of products is not guaranteed, duplicity cannot be prevented.

Although similar tools to those abovementioned can be found worldwide, their distinctive feature lies in the fact that they are founded on robust and truthful databases, as is the case of scriptLattes [@MenaChalcoJunior2009]: an open-source system to create academic reports of groups based on curricula of the Lattes Database (the Brazilian equivalent of CvLAC). This particular database is one of the most robust in Latin America, as shown in Table 1.

More specific cases include platforms that merely assess productivity using Scopus and WoS databases, thus hindering an accurate analysis of institutions' productivity as not all the items included in academic research CV systems are contemplated [@RuizRosero2019; @DattoloCorbatto2022; @Patil2020].

Due to all of the above, the objective of this work is to evaluate the data recorded in the Colombian academic CV system and to validate its accuracy by cross-checking with international sources through an automated tool. For this purpose, an ETL (extraction, transformation, and loading) data integration process from the source system has been completed, placing a special emphasis on scientific articles as they can be easily verified using international databases. Subsequently, the software architecture tool is introduced, along with the algorithms and components required for its analysis and validation. This work highlights the need to implement activities aimed at improving the review of data quality and integrity in national information systems, with a special focus on CvLAC.

## Methodology

A six-phase methodology was outlined to carry out this work.

### Data extraction

This stage involved evaluating the structure used to present the CV information in the CvLAC platform and identifying possible scenarios during data extraction. Moreover, given the amount of information registered in the CVs, only author information, identifiers and work experience were considered. Finally, a specific data extraction tool named CvLAC Scraper was developed using Python and the Scrapy framework (Martinez et al., 2019).

### Data cleaning and standardization

After developing the process and the extraction tool, a data representation standard was defined using the JSON data format. During the cleaning process, regular expressions were used to remove inconsistencies in the extracted data (e.g., unwanted characters), to correct inconsistent formats and to normalize the information.

### Verifying data reliability

An exploratory analysis of the extracted data was conducted, and metrics were defined to evaluate both consistency and completeness. This validation process involved verifying the reported ORCIDs and DOIs as well as related data. The goal was to compare the information extracted from the CVs with international sources.

### Implementing the tool

Software lifecycle stages were employed to develop an author information search strategy and a verification tool. This resulted in a three-layer application with Angular, Flask and MongoDB, and the necessary functionalities to perform searches, verifications, and analysis regarding author information.

## Results

One of the biggest challenges faced was the vast amount of CVs and data registered in CvLAC, consequently revealing one of its flaws: lack of control concerning registration. Registries with inconsistent numbers pointing to non-existent documents (e.g., "00000"), or names such as "Prueba Uno" (Test 1) entailed obtaining individual access to profiles. Since the URLs included in each CV contain a researcher ID (cod_rh), it is only necessary to change such code on the base URL to gain access to a researcher's information.

The HTML structure used in CV websites was analyzed during the data extraction process. @tbl-cvlac2 contains the information regarding every search conducted in XML Path Language, together with the name used to identify it. As a means to explain the employed notation and simplify searches, note that the term BASE_XPATH refers to the beginning of each search and that those labeled with ellipses refer to the rest of the search.

```{=html}

<table id="tbl-cvlac2">
  <caption>Xpath searches to access information</caption>
  <colgroup>
    <col width="35%"> <!-- Ancho para la columna "Name" -->
    <col width="65%"> <!-- Ancho para la columna "XPATH/Description" -->
  </colgroup>
  <thead>
    <tr>
      <th><strong>Name</strong></th>
      <th><strong>XPATH/Description</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>BASE_XPATH</td>
      <td>It is used to access CV information with no restricted data.<br><code>/html/body/div/div[3]/table/tbody</code></td>
    </tr>
    <tr>
      <td>AUTHOR_INFO</td>
      <td>It selects the section of the table with general information.<br><code>.../tr[2]/td/table/tbody</code></td>
    </tr>
    <tr>
      <td>AUTHOR_IDS</td>
      <td>It is used to navigate between table rows containing identifiers.<br><code>.../tr[4]/td/table/tbody/tr/td/a</code></td>
    </tr>
    <tr>
      <td>SOCIAL_MEDIA</td>
      <td>It is used to navigate between the table rows containing social media.<br><code>.../tr[3]/td/table/tbody/tr/td/a</code></td>
    </tr>
    <tr>
      <td>ARTICLES</td>
      <td>It selects the table containing the articles.<br><code>.../tr[45]/td/table/tbody</code></td>
    </tr>
    <tr>
      <td>ARTICLE_TYPE</td>
      <td>It is used to obtain the headers of the articles, <em>i.e.</em>, the verification mark and the type of article.<br><code>.../tr[45]/td/table/tbody/tr[position() mod 2 = 0]/td/li</code></td>
    </tr>
    <tr>
      <td>ARTICLE_INFO</td>
      <td>It grants access to the internal HTML of each article.<br><code>.../tr[45]/td/table/tbody/tr/td/blockquote</code></td>
    </tr>
    <tr>
      <td>ARTICLE_SECTION_INFO</td>
      <td>It is used, together with the previous XPATH, to access the different fields of an article. Each section is numbered consecutively: 1) name, authors, country; 2) journal name; 3) ISSN; 4) publisher; 5) volume; 6) issue, starting page, final page, year; 7) DOI; 8) keywords; 9) sections; 10) acknowledgements.<br><code>text()[{}]</code></td>
    </tr>
    <tr>
      <td>NAME_WORK_PLACES</td>
      <td>It is used to access the name of the institution or places where the researcher has worked.<br><code>.../tr[8]/td/table/tbody/tr/td[2]/b/text()</code></td>
    </tr>
    <tr>
      <td>TIME_WORK_PLACES</td>
      <td>It provides start and end dates of work experience.<br><code>.../tr[8]/td/table/tbody/tr/td[2]/i[contains(text(),<br>"Dedicación:")/following-sibling::text()[1]</code></td>
    </tr>
    <tr>
      <td>CHECK_RESTRICTED_INFO</td>
      <td>It is used to obtain the following text: "The information in this curriculum is not available at the researcher's request."<br><code>/html/body/table/tbody/tr[3]/td/blockquote/text()</code></td>
    </tr>
    <tr>
      <td>AUTHOR_RESTRICTED_INFO</td>
      <td>This search grants access to the table containing general information about researchers who have restricted their information.<br><code>/html/body/table/tbody/tr[2]/td/blockquote/table/tbody</code></td>
    </tr>
  </tbody>
</table>

```

Even though CV data extraction was successfully obtained from the various searches shown in @tbl-cvlac2, lack of standardization regarding the input of information, especially data on scientific publications and work experience, made it necessary to use regular expressions designed to work with the HTML structure of web pages. As a result, data could be effectively processed.

@tbl-cvlac3 shows some of the expressions used in data cleaning, standardization and extraction.

```{=html}

<table id="tbl-cvlac3">
  <caption>Regular expressions used in data extraction and cleaning</caption>
  <colgroup>
    <col width="35%"> <!-- Ancho para la columna "Name" -->
    <col width="25%"> <!-- Ancho para la columna "Regular expression" -->
    <col width="40%"> <!-- Ancho para la columna "Description" -->
  </colgroup>
  <thead>
    <tr>
      <th><strong>Name</strong></th>
      <th><strong>Regular expression</strong></th>
      <th><strong>Description</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>REMOVE_SPECIAL<br>_SPACE_CHARACTERS</td>
      <td><code>([\n\r\t]|+|\xa0)</code></td>
      <td>It removes line breaks, tabs and special characters from the HTML structure of a curriculum.</td>
    </tr>
    <tr>
      <td>GET_ARTICLE_TITLE</td>
      <td><code>".+"</code></td>
      <td>It obtains the complete name of an article.</td>
    </tr>
    <tr>
      <td>GET_PAGES_STRING</td>
      <td><code>p\. *\w+-?\w* - +\w+-?\w*</code></td>
      <td>It obtains the first and last page of an article.</td>
    </tr>
    <tr>
      <td>GET_PAGES</td>
      <td><code>\w+-?\w*</code></td>
      <td>It obtains the first and last page of a specific text string.</td>
    </tr>
    <tr>
      <td>GET_YEARS</td>
      <td><code>\d{4}</code></td>
      <td>It determines the years of work experience.</td>
    </tr>
    <tr>
      <td>GET_AUTHOR_ID</td>
      <td><code>\d+$</code></td>
      <td>It extracts the identifier or cod_rh from the URL in a curriculum.</td>
    </tr>
    <tr>
      <td>HTML_TAGS</td>
      <td><code>&lt;\/?w+&gt;</code></td>
      <td>It extracts the HTML labels contained in a text string or document.</td>
    </tr>
  </tbody>
</table>

```

JSON was the preferred format to design the data representation schema because data can be sent via HTTP requests between web browsers and other applications. @fig-json shows the schemas applied to the information collected from CVs. It is important to observe that when fields are left unspecified, it is highly probable that null or absent values are present in the schemas.

![JSON schemas representing author and article](img/json-schemas.png){#fig-json fig-pos="htbp"}

As aforementioned in the Methodology section, the CvLAC Scraper tool was developed. @fig-architecture presents the general architecture defined for its configuration, execution and output data.

![CvLAC Scraper architecture](img/cvlac-scraper-architecture.png){#fig-architecture fig-pos="htbp"}

CvLAC Scraper has an initial configuration file to retrieve information from CV sites. It defines aspects such as CvLAC URL, HTML input types and CV identifiers, and data persistence, to name but a few. (@fig-config).

![CvLAC Scraper configuration file](img/cvlac-scraper-config.png){#fig-config fig-pos="htbp"}

A data processing pipeline extracts data from an author's CV using regular expressions in XPATH searches, and the defined structure. Once the CV data extraction is complete, the processed data is sent to the next pipeline in the form of items or dictionaries.

After processing the data on a CV, structured data items are stored in a storage pipeline until a determined number to be stored per batch is reached. Once that number is reached, the persistence process starts and the memory space is released so that other items can be stored.

Depending on the specified configuration, this process may vary as data can be stored in any previously determined format or database.

## Discussion

Once the data extracted from CvLAC was examined, these were compared using API platforms like Crossref. A feature that sets this platform apart from others is that a total of 2,117,500 profile identifiers was obtained (https://cvlac-verifier.onrender.com/home). Only 995,822 of them were linked to a curriculum, therefore, the remaining profiles are either empty or contain restricted information. Results collected highlight that less than 50% are linked to a DOI, as shown in @fig-results. Considering that a small proportion of DOIs is invalid makes the overall situation even more critical.

![Results of data extraction](img/data-extraction-results.png){#fig-results fig-pos="htbp"}

An issue regarding registration arose while analyzing data: a recorded identifier may not correspond to the information inserted into that blank space. As a case in point, researchers enter their ScopusID into an ORCID blank, or vice versa.

The public ORCID API used to assess the validity of the registered ORCIDs verifies if registered data corresponds to an existing ORCID, and, if such is the case, determines whether it corresponds to the authors who have registered it.

Even though there were 56,918 ORCID records, it was only possible to evaluate 39,792 after eliminating inconsistent data records (e.g., hyphens or other characters). Among these, 1,994 identifiers did not have an ORCID defined structure, particularly the 16-digit number, which is why they could not be verified. Out of the remaining 37,798, 86% (34,290) were found to be valid. The rest did not return a valid or active registry in the portal.

Similarity measures needed to be designed and used in the validation process so as to examine the characters that compose the researchers' names within CvLAC and ORCID. This was performed after a prior data cleaning process that involved removing or replacing punctuation marks. An 80% coincidence was applied for a record to be recognized as valid. Some registries can be falsely included as valid due to homonyms or common names.

Information about 790,742 registered articles was extracted from the CvLAC database, out of which 37.42% have an associated DOI and 20.63% have one that is valid and checked using the DOI Foundation API. Moreover, 13.15% have a valid and unique DOI (@fig-doi). In this regard, it was found that different articles were registered on CvLAC with the same associated DOI, which is not right.

![Validating the existence of articles using DOIs](img/doi-validation.png){#fig-doi fig-pos="htbp"}

Although the first phase of data evaluation shows a significant amount of erroneous information affecting the country's indicators, it is important to analyze researchers.

Using the Crossref Metadata API facilitated the validation of the authenticity of data registered in the articles, via their identifier (DOI) information was accessed in the database. First, we verified the coincidence percentage between article titles in CvLAC and Crossref, which showed a high level of agreement (@fig-titles). Although only a small difference was detected in certain cases, it should not be overlooked in order to correctly validate data.

![Coincidences in article titles: Crossref vs. CvLAC](img/title-coincidences.png){#fig-titles fig-pos="htbp"}

Moreover, @tbl-cvlac4 summarizes the results obtained from the validation process of author names per article registered in CvLAC contrasted to those in Crossref. When submitting articles to CvLAC, author names are entered following a specific format: full name, only initials, only a name and a surname. In consequence, generating several combinations for each author name became essential mainly because citation styles, including whether authors' names are either abbreviated, are not dictated by Crossref.

| **% of articles** | **Authors in CvLAC** | **Authors in Crossref** | **Validated authors (%)** |
|------------------|---------------------|------------------------|---------------------------|
| 5 % | 1 | 2 | 5.26 |
| 20 % |   |   | 20.0 |
| 40 % |   | 3 | 50.0 |
| 50 % | 2 | 4 | 66.67 |
| 60 % |   |   | 100 |
| 80 % | 4 | 7 |   |
| 95 % | 6 | 14 |   |

: Author validation per article. {#tbl-cvlac4}

@tbl-cvlac4 indicates that 40% of the verified articles show a 100% data agreement between both systems. However, there is a difference of at least one author in 60% of the articles that affects the cooperation and collaboration figures used by the Ministry of Science, Technology and Innovation to measure national research capability. Studying collaboration networks provides insights that help in improving the researcher's profile and coordinator's productivity of research [@Razzaq2022]. In some cases, such as CovidSurg [@MinisterioCienciaTecnologia2020], there are articles where 16,148 authors are registered in Crossref and only one in CvLAC. Verifying this figure revealed it was related to products from organizations with thousands of collaborators worldwide.

Dublin Core metadata standard was used to evaluate both the quality and integrity of data [@COVIDSurgCollaborative2022]. Additionally, a score following such standard was assigned to each field, with the exception of the description and relational fields (@tbl-cvlac5).

| **Item** | **Total Score** | **Criteria** | **Individual Score** |
|----------|----------------|--------------|---------------------|
| Title | 8 | Not established | 0 |
|   |   | Established | 2 |
|   |   | Verified | 6 |
| Author or Creator | 8 | Not established | 0 |
|   |   | Established | 2 |
|   |   | Verified | 6 |
| Subject and Keywords | 4 | Not established | 0 |
|   |   | Established | 4 |
| Publisher | 8 | Not established | 0 |
|   |   | Established | 2 |
|   |   | Verified | 6 |
| Other Contributor | 4 | Not established | 0 |
|   |   | Established | 4 |
| Date | 8 | Not established | 0 |
|   |   | Established | 2 |
|   |   | Verified | 6 |
| Resource Type | 4 | Not established | 0 |
|   |   | Established | 4 |
| Format | 4 | Not established | 0 |
|   |   | Established | 4 |
| Resource Identifier | 20 | Not established | 0 |
|   |   | Established | 4 |
|   |   | Verified | 8 |
|   |   | Verified and unique | 8 |
| Source | 24 | Journal | 12 (3 for each field) |
|   |   | Volume |   |
|   |   | Title page |   |
|   |   | Last Page |   |
|   |   | Contains ISSN | 4 |
|   |   | Verified ISSN | 8 |
| Language | 4 | Not established | 0 |
|   |   | Established | 4 |
| Coverage | 4 | Not established | 0 |
|   |   | Established | 4 |
| Total | 100 |   | 100 |

: Metadata evaluation metrics. {#tbl-cvlac5}

This evaluation revealed that 60.31% of the articles had a score of 32-36. In addition, no article reached a score of 100; as a matter of fact, the maximum value obtained was 61. Not only does this stem from challenges CvLAC confronts when fields regarding language and format cannot be web scraped, but also from the fact that fields concerning other contributors and coverage are not required by the platform.

After reviewing the score percentage reached in each field (@fig-metadata), resource identifiers (DOIs) had the lowest fulfillment percentage because a large number of articles are missing them. As a result, it was impossible to verify their truthfulness. Since metadata like subject and keywords are no longer mandatory, authors are unlikely to provide them.

![Score obtained for each Dublin Core metadata item](img/metadata-scores.png){#fig-metadata fig-pos="htbp"}

## Conclusions

As is the case with most regional information systems for scientific research in Latin America, CvLAC allows the registration and consultation of data. Nonetheless, it is vital to seize the opportunity to improve its functionality by implementing a metadata standard like Dublin Core to standardize data structure, applying author identifier validation criteria, and using external APIs to ensure the existence of reported products.

An information system for a country's scientific production aims at providing indicators that offer a comprehensive view of research capabilities and regional potential. This type of system becomes unreliable if the registered information is not correctly validated.

When focusing on CvLAC, we noticed that a lack of international standards and connections to external databases can lead to information gaps, inadequate nationwide decision-making and investments in areas that are inconsistent with the national outlook. This research helps detect errors and contributes to improving governmental research support systems.